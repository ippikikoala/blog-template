---
title: "「生成AIアプリケーション開発入門」を読んでみて（Part1）"
date: "2025-11-06"
description: "- LLMの設定(LLMの設定) - Temperature(Temperature) - Top PとTop K(Top-PとTop-K) - Max Tokens(Max-Tokens) - Stop Sequences(Stop-Sequences) - Frequency Penalty(F..."
category: "IT"
tags: []
image: ""
---

- [LLMの設定](#LLMの設定)

            - [Temperature](#Temperature)

            - [Top PとTop K](#Top-PとTop-K)

            - [Max Tokens](#Max-Tokens)

            - [Stop Sequences](#Stop-Sequences)

            - [Frequency Penalty](#Frequency-Penalty)

            - [Seed](#Seed)




    - [LLMの選定（おまけ）](#LLMの選定おまけ)

生成AIは使っているけど実際よくわかってないという不安に駆られ、業務でもAIをちゃんと使っていこう！（遅い）という方針になったので読んでみたのが「生成AIアプリケーション開発入門」。まだ最初の方しか読んでいないけどAIの基本みたいなのがよくわかってよい。個人的には「AIを業務で使う」と「AIを業務でちゃんと使う」の間にはすげーレベル差があると感じていて、この本は後者のかなり実践的なところまで扱っていそうだったので購入してみた。

プロンプトが大切！深津式がいい！もう古い！とか色々聞くけど、自分はプロンプトのどこにどういう意味があってなんでこんな書き方するんだ？とか色々気になる面倒な性格なのである。この本を読むことで疑問が少し解消していると思う。

特に印象的だったのはLLMの進化とLLMの設定。

LLM=Large Language Modelで大規模[言語モデル](https://d.hatena.ne.jp/keyword/%B8%C0%B8%EC%A5%E2%A5%C7%A5%EB)のこと（ちなみに特定の分野に特化させた使い方をするようなSLMもある）。

Copilotの右上に「GPT云々を使用中」とか書いてあるけどあれが代表的なLLMであるGPT。ClaudeとかGeminiもLLMの一つ。

生成AIって言うけどLLMがその構成要素の一つになっている。人の言葉（プロンプト）を解釈して自然なテキストを生成する役割を担っているのがLLM。

#### LLMの進化

LLMが進化した！というときに言われるものを抜粋してみた（AIって理論的には大昔にある程度完成していて、CPU、[GPU](https://d.hatena.ne.jp/keyword/GPU)の進化という[脳筋](https://d.hatena.ne.jp/keyword/%C7%BE%B6%DA)パワープレイで最近のし上がってきたやつという偏見は消えないのだが）

##### マルチモーダル

最近のChatGPTとかだと当たり前になっているが、テキストだけではなく音声や画像をあわせて理解・生成できる機能

##### トークンの増大

[トーク](https://d.hatena.ne.jp/keyword/%A5%C8%A1%BC%A5%AF)ンっていろんな文脈で出てくるので難しい。。。文章をモデルが理解できる単位に分割したものを[トーク](https://d.hatena.ne.jp/keyword/%A5%C8%A1%BC%A5%AF)ンと呼ぶ。

「いっぴきこあらの大冒険です」なら、「いっぴき」「こあら」「の」「大」「冒険」になるのかな？AIが思考するために使える情報量も[トーク](https://d.hatena.ne.jp/keyword/%A5%C8%A1%BC%A5%AF)ンと呼ばれるらしい。なので[トーク](https://d.hatena.ne.jp/keyword/%A5%C8%A1%BC%A5%AF)ンの増大とは思考に使えるコンテキストが増えたということ。最近のモデルでは128K以上はあり、10000語以上の文脈を思考のコンテキストとして使える。

ちなみに入力テキストと出力テキストも[トーク](https://d.hatena.ne.jp/keyword/%A5%C8%A1%BC%A5%AF)ンとして管理され、例えばOpenAIでは入力と出力の[トーク](https://d.hatena.ne.jp/keyword/%A5%C8%A1%BC%A5%AF)ンを合わせたもので課金される。

[トーク](https://d.hatena.ne.jp/keyword/%A5%C8%A1%BC%A5%AF)ン量によってどれだけ入力を食わせられるかが決まるから意識すべきパラメータと理解している

##### パラメータの増大

[ニューラルネットワーク](https://d.hatena.ne.jp/keyword/%A5%CB%A5%E5%A1%BC%A5%E9%A5%EB%A5%CD%A5%C3%A5%C8%A5%EF%A1%BC%A5%AF)を理解できていないのだけど、点と点を結ぶ「ノード」の数と重み、バイアスをパラメータと呼ぶらしく、それが増えたと考えることができるらしい。ここらへんは全然理解できていない。。。パラメータ数が増大するほど[ニューラルネットワーク](https://d.hatena.ne.jp/keyword/%A5%CB%A5%E5%A1%BC%A5%E9%A5%EB%A5%CD%A5%C3%A5%C8%A5%EF%A1%BC%A5%AF)は繊細になる

#### LLMの設定

プロンプトを作る段階で意識したほうがいいと思っている。というか世の簡単な「プロンプトエンジニアリング」ってこれらのパラメータを意識して[言語化](https://d.hatena.ne.jp/keyword/%B8%C0%B8%EC%B2%BD)しているだけに感じてる。

特徴としてCopilotでここらを直接いじることはできないらしい。だからこそ上記のようなプロンプトで間接的にいじってやる必要がある。

一方でOpenAIはこれらを直接指定してやることができる。専門的に使うならOpenAI一択なのでは？という気分。ていうかCopilotってモデルにぶち込むためのモデルみたいなのを作っているだけなのでは？という推測

##### Temperature

生成されるテキストの多様性を制御するパラメータ。低い値だと[決定論](https://d.hatena.ne.jp/keyword/%B7%E8%C4%EA%CF%C0)的に、高い値だと多様になる。

例えばCopilotだと現状は直接指定できないがプロンプトで間接的に決定できる。

ex.)温度の高い書き方：創造的に回答してください。多少のずれは許容します

温度の低い書き方：簡潔に事実に基づいて一貫した回答をしてください

Open AIにはTemperatureをしている[API](https://d.hatena.ne.jp/keyword/API)があるっぽい。

##### Top PとTop K

生成AIが次の単語を決めるときにどう決めるかを決定する方法。例えば「あの犬は」に続く文章を考えてください。と聞いたとき、AIはいくつかの候補を内部的に出す。

( )内は確率として、

かわいい（0.5）、かっこいい（0.2）、うるさい（0.1）、たかそう（0.05）、くさい（0.01）みたいな...

Top Pは確率を意味していて、確率の合計が一定になった候補の中からランダムに選ぶ。例えばP=0.8だったら、かわいい（0.5）、かっこいい（0.2）、うるさい（0.1）の確率の合計が0.8なので、この3つが候補にあがり、その中からランダムに選ばれる。

一方、Top Kは個数を表していて、上位K個の中からランダムに選ぶ。例えばK=4なら、かわいい（0.5）、かっこいい（0.2）、うるさい（0.1）、たかそう（0.05）の4子が候補にあがり、その中からランダムに選ばれる。

##### Max Tokens

これはシンプルで、生成するテキストの最大の[トーク](https://d.hatena.ne.jp/keyword/%A5%C8%A1%BC%A5%AF)ン量

##### Stop Sequences

生成を停止するシーケンス

例えば11にすると項目が10に達したら打ち切る感じ。これは「3つ理由を回答してください」みたいなプロンプトで無意識に使っているかも

##### Frequency Penalty

繰り返しを減らすように働くパラメータ。AIはうんぬん、AIはあれこれ、みたいな言葉の繰り返しを避けるために使える

##### Seed

同じシード値を使うと同じ生成をすることができる。乱数を固定することができるらしい。

Copilotでは設定できない

#### LLMの選定（おまけ）

### モデルサイズ**

SLMも場合によっては良い。特定の領域に特化したものもある

### 学習データ**

モデルデータがどこの時点の情報まで含んでいるか把握することは大切

### [トーク](https://d.hatena.ne.jp/keyword/%A5%C8%A1%BC%A5%AF)ン数**

入出力[トーク](https://d.hatena.ne.jp/keyword/%A5%C8%A1%BC%A5%AF)ン数

### マルチモーダル**

### コスト**

例えばOpen AIでは入力と出力[トーク](https://d.hatena.ne.jp/keyword/%A5%C8%A1%BC%A5%AF)ンを合わせたもので課金される

### 推論速度**
