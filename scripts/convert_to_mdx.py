#!/usr/bin/env python3
"""
ã¯ã¦ãªãƒ–ãƒ­ã‚°ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’MDXå½¢å¼ã«å¤‰æ›

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™:
1. BODYã¨EXTENDED BODYã®çµ±åˆï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„é‡è¤‡é˜²æ­¢ï¼‰
2. ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆul style="list-style: none"ï¼‰ã®æ­£ã—ã„å¤‰æ›
3. è¦‹å‡ºã—ã‚¿ã‚°ã®ç”»åƒãƒ»HTMLå‰Šé™¤
4. figure+img+figcaptionã®é©åˆ‡ãªå¤‰æ›
5. HTMLã‚¿ã‚°ã®å®Œå…¨å‰Šé™¤
6. MDXæ³¢æ‹¬å¼§ï¼ˆ{}ï¼‰ã®ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
7. ç”»åƒURLã®Cloudflare R2ã¸ã®å¤‰æ›
8. descriptionãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

Usage:
    python3 scripts/convert_to_mdx.py <export_file> [r2_public_url]

Example:
    python3 scripts/convert_to_mdx.py ippikikoala.hatenablog.com.export.txt
    python3 scripts/convert_to_mdx.py ippikikoala.hatenablog.com.export.txt https://pub-xxxxx.r2.dev

Requirements:
    - hatena_images/image_mapping.txt ãŒå­˜åœ¨ã™ã‚‹ã“ã¨
    - R2ã®Public URLãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨

Output:
    - content/posts/*.mdx ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã™
    - æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã¯ .bak ã¨ã—ã¦è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã•ã‚Œã¾ã™

Last Updated: 2026-01-07
Tested: 137 articles migrated successfully with 0 errors
"""

import re
import sys
import os
from datetime import datetime
from pathlib import Path

def parse_hatena_export(export_file):
    """ã¯ã¦ãªãƒ–ãƒ­ã‚°ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹"""

    with open(export_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # è¨˜äº‹ã”ã¨ã«åˆ†å‰²ï¼ˆ--------ã§åŒºåˆ‡ã‚‰ã‚Œã¦ã„ã‚‹ï¼‰
    entries = content.split('--------')

    posts = []

    for entry in entries:
        if not entry.strip():
            continue

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åˆ†é›¢
        if '-----' not in entry:
            continue

        parts = entry.split('-----')
        if len(parts) < 2:
            continue

        metadata_section = parts[0]

        # BODYã¨EXTENDED BODYã‚’çµ±åˆ
        body = ''
        extended_body = ''

        for i, part in enumerate(parts):
            # BODY: ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
            if 'BODY:' in part:
                body_content = part.split('BODY:')
                if len(body_content) > 1:
                    body = body_content[1].strip()

            # EXTENDED BODY: ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
            if 'EXTENDED BODY:' in part and '-----' in part:
                # 'EXTENDED BODY:' ã‹ã‚‰æ¬¡ã® '-----' ã¾ã§ã‚’å–å¾—
                extended_parts = part.split('EXTENDED BODY:')
                if len(extended_parts) > 1:
                    extended_body = extended_parts[1].strip()
                    break  # æœ€åˆã®EXTENDED BODYã®ã¿å‡¦ç†

        # BODYã¨EXTENDED BODYã‚’çµåˆ
        full_body = body
        if extended_body:
            full_body = body + '\n' + extended_body

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹
        post = {}
        for line in metadata_section.split('\n'):
            if ':' in line:
                key, value = line.split(':', 1)
                post[key.strip().lower()] = value.strip()

        if full_body:
            post['body'] = full_body
            posts.append(post)

    return posts

def convert_to_mdx(post, image_mapping, r2_public_url):
    """ã¯ã¦ãªãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’MDXå½¢å¼ã«å¤‰æ›"""

    # æ—¥ä»˜å¤‰æ›
    date_str = post.get('date', '')
    try:
        dt = datetime.strptime(date_str, '%m/%d/%Y %H:%M:%S')
        date = dt.strftime('%Y-%m-%d')
    except:
        date = '2025-01-01'

    # ã‚¹ãƒ©ãƒƒã‚°ç”Ÿæˆ
    basename = post.get('basename', '')
    if not basename:
        # basenameãŒãªã„å ´åˆã¯ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ç”Ÿæˆ
        title = post.get('title', 'untitled')
        basename = re.sub(r'[^a-z0-9]+', '-', title.lower())
    else:
        # basenameã« "/" ã‚„ãã®ä»–ã®ç‰¹æ®Šæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ç½®æ›
        basename = re.sub(r'[^a-z0-9]+', '-', basename.lower())

    slug = f"{date}-{basename}"

    # ã‚«ãƒ†ã‚´ãƒªãƒ»ã‚¿ã‚°
    category = post.get('category', '')
    tags_str = post.get('tags', '')
    tags = [tag.strip() for tag in tags_str.split(',') if tag.strip()]

    # æœ¬æ–‡å‡¦ç†
    body = post.get('body', '')

    # ã¯ã¦ãªè¨˜æ³•ã‚’å¤‰æ›
    body = convert_hatena_syntax(body, image_mapping, r2_public_url)

    # èª¬æ˜æ–‡ã‚’æŠ½å‡º
    description = extract_description(body)

    # ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ä½œæˆ
    title = post.get('title', 'Untitled').replace('"', '\\"')
    description = description.replace('"', '\\"')

    frontmatter = f"""---
title: "{title}"
date: "{date}"
description: "{description}"
category: "{category}"
tags: {tags}
image: ""
---

{body}
"""

    return slug, frontmatter

def convert_hatena_syntax(body, image_mapping, r2_public_url):
    """ã¯ã¦ãªè¨˜æ³•ã¨HTMLã‚’Markdownã«å¤‰æ›"""
    import html

    # 1. ç›®æ¬¡ã®å‰Šé™¤ï¼ˆMDXã§è‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹ãŸã‚ï¼‰
    body = re.sub(r'<ul class="table-of-contents">.*?</ul>', '', body, flags=re.DOTALL)

    # 1.5. ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆul style="list-style: none"ï¼‰ã‚’Markdownã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã«å¤‰æ›
    def convert_code_block(match):
        ul_content = match.group(1)
        # li ã‚¿ã‚°ã‹ã‚‰ã‚³ãƒ¼ãƒ‰è¡Œã‚’æŠ½å‡º
        lines = re.findall(r'<li>(.*?)</li>', ul_content, re.DOTALL)
        # span ã‚¿ã‚°ãªã©ã®HTMLã‚’å‰Šé™¤
        cleaned_lines = []
        for line in lines:
            # span ã‚¿ã‚°ã‚’å‰Šé™¤
            line = re.sub(r'<span[^>]*>', '', line)
            line = re.sub(r'</span>', '', line)
            # ãã®ä»–ã®HTMLã‚¿ã‚°ã‚’å‰Šé™¤
            line = re.sub(r'<[^>]+>', '', line)
            # HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
            line = html.unescape(line)
            cleaned_lines.append(line)
        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¨ã—ã¦å‡ºåŠ›
        code_content = '\n'.join(cleaned_lines)
        return f'\n\n```\n{code_content}\n```\n\n'

    body = re.sub(r'<ul style="list-style: none[^"]*"[^>]*>(.*?)</ul>', convert_code_block, body, flags=re.DOTALL)

    # 2. YouTubeåŸ‹ã‚è¾¼ã¿ã‚’MDXã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«å¤‰æ›
    def convert_youtube_iframe(match):
        iframe_content = match.group(0)
        # srcå±æ€§ã‹ã‚‰videoIdã‚’æŠ½å‡º
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: https://www.youtube.com/embed/VIDEO_ID
        video_id_match = re.search(r'youtube\.com/embed/([a-zA-Z0-9_-]+)', iframe_content)
        if video_id_match:
            video_id = video_id_match.group(1)
            # titleå±æ€§ãŒã‚ã‚Œã°å–å¾—
            title_match = re.search(r'title="([^"]*)"', iframe_content)
            title = title_match.group(1) if title_match else ""
            if title:
                return f'\n\n<YouTube videoId="{video_id}" title="{title}" />\n\n'
            else:
                return f'\n\n<YouTube videoId="{video_id}" />\n\n'
        return ''

    body = re.sub(r'<iframe[^>]*youtube\.com/embed[^>]*>.*?</iframe>', convert_youtube_iframe, body, flags=re.DOTALL)

    # 2.5. ã¯ã¦ãªãƒ–ãƒ­ã‚°ã®åŸ‹ã‚è¾¼ã¿ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å‰Šé™¤
    body = re.sub(r'<iframe[^>]*src="https://hatenablog-parts\.com/embed[^>]*>.*?</iframe>', '', body, flags=re.DOTALL)
    body = re.sub(r'<iframe[^>]*src="https://www\.google\.com/maps/embed[^>]*>.*?</iframe>', '\n\n[Google MapsåŸ‹ã‚è¾¼ã¿]\n\n', body, flags=re.DOTALL)

    # 3. ã¯ã¦ãªç‰¹æœ‰ã®ã‚¿ã‚°ã‚’å‰Šé™¤
    body = re.sub(r'<cite class="hatena-citation">.*?</cite>', '', body, flags=re.DOTALL)

    # 4. è¦‹å‡ºã—ã‚¿ã‚°: <h3>, <h4>, <h5> â†’ ###, ####, #####
    # è¦‹å‡ºã—å†…ã®ç”»åƒã‚¿ã‚°ã‚„æ”¹è¡Œã‚¿ã‚°ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰MarkdownåŒ–ï¼ˆç”»åƒå‡¦ç†ã®å‰ã«å®Ÿè¡Œï¼‰
    def clean_heading(match):
        tag = match.group(1)  # h3, h4, h5
        content = match.group(2)
        # è¦‹å‡ºã—å†…ã®ç”»åƒã‚¿ã‚°ã‚’å‰Šé™¤
        content = re.sub(r'<img[^>]*>', '', content)
        # è¦‹å‡ºã—å†…ã®æ”¹è¡Œã‚¿ã‚°ã‚’å‰Šé™¤
        content = re.sub(r'<br\s*/?\s*>', ' ', content, flags=re.IGNORECASE)
        # è¦‹å‡ºã—å†…ã®ãƒªãƒ³ã‚¯ã‚¿ã‚°ã¯ãƒ†ã‚­ã‚¹ãƒˆã®ã¿æŠ½å‡º
        content = re.sub(r'<a[^>]*>(.*?)</a>', r'\1', content, flags=re.DOTALL)
        # è¦‹å‡ºã—å†…ã®ãã®ä»–ã®HTMLã‚¿ã‚°ã‚’å‰Šé™¤
        content = re.sub(r'<[^>]+>', '', content)
        # å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
        content = content.strip()
        level = '#' * (int(tag[1]) + 1)  # h3â†’###, h4â†’####, h5â†’#####
        return f'\n\n{level} {content}\n\n'

    body = re.sub(r'<(h3)[^>]*>(.*?)</h3>', clean_heading, body, flags=re.DOTALL)
    body = re.sub(r'<(h4)[^>]*>(.*?)</h4>', clean_heading, body, flags=re.DOTALL)
    body = re.sub(r'<(h5)[^>]*>(.*?)</h5>', clean_heading, body, flags=re.DOTALL)

    # 5. figure + img + figcaption â†’ ç”»åƒ + ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³
    def replace_figure(match):
        full_figure = match.group(0)
        # figcaptionã‹ã‚‰ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
        caption_match = re.search(r'<figcaption[^>]*>(.*?)</figcaption>', full_figure, re.DOTALL)
        caption = caption_match.group(1).strip() if caption_match else ''
        # imgã‚¿ã‚°ã‚’æŠ½å‡º
        img_match = re.search(r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>', full_figure)
        if img_match:
            src = img_match.group(1)
            # ç”»åƒURLã‚’R2ã«å¤‰æ›
            basename = os.path.basename(src).rsplit('.', 1)[0]
            if src in image_mapping:
                filename = image_mapping[src]
                new_path = f"{r2_public_url}/posts/{filename}"
            elif basename in image_mapping:
                filename = image_mapping[basename]
                new_path = f"{r2_public_url}/posts/{filename}"
            else:
                new_path = src
            # Markdownå½¢å¼ã§å‡ºåŠ›ï¼ˆã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã¯alt textã¨ã—ã¦ä½¿ç”¨ï¼‰
            if caption:
                result = f"\n\n![{caption}]({new_path})\n\n_{caption}_\n\n"
            else:
                result = f"\n\n![ç”»åƒ]({new_path})\n\n"
            return result
        return ''

    body = re.sub(r'<figure[^>]*>.*?</figure>', replace_figure, body, flags=re.DOTALL)

    # 5. æ®‹ã£ãŸç”»åƒã‚¿ã‚°: <img> â†’ ![](...)
    def replace_html_image(match):
        src = match.group(1)
        # URLãŒãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ã‚‹å ´åˆã€R2ã®URLã«å¤‰æ›
        if src in image_mapping:
            filename = image_mapping[src]
            new_path = f"{r2_public_url}/posts/{filename}"
            return f"![ç”»åƒ]({new_path})"
        # URLã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡ºã—ã¦æ¤œç´¢
        basename = os.path.basename(src).rsplit('.', 1)[0]
        if basename in image_mapping:
            filename = image_mapping[basename]
            new_path = f"{r2_public_url}/posts/{filename}"
            return f"![ç”»åƒ]({new_path})"
        # ãƒãƒƒãƒ”ãƒ³ã‚°ã«ãªã‘ã‚Œã°å…ƒã®URLã‚’ãã®ã¾ã¾ä½¿ç”¨
        return f"![ç”»åƒ]({src})"

    body = re.sub(r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>', replace_html_image, body)

    # 6. ç”»åƒ: [f:id:...] â†’ ![](...)
    def replace_fid_image(match):
        image_id = match.group(1)
        if image_id in image_mapping:
            filename = image_mapping[image_id]
            new_path = f"{r2_public_url}/posts/{filename}"
            return f"![ç”»åƒ]({new_path})"
        return match.group(0)

    body = re.sub(r'\[f:id:([^\]]+)\]', replace_fid_image, body)

    # 7. ãƒªãƒ³ã‚¯: <a> â†’ [text](url)
    body = re.sub(r'<a[^>]*class="keyword"[^>]*href=["\']([^"\']+)["\'][^>]*>(.*?)</a>', r'[\2](\1)', body, flags=re.DOTALL)
    body = re.sub(r'<a[^>]*href=["\']([^"\']+)["\'][^>]*>(.*?)</a>', r'[\2](\1)', body, flags=re.DOTALL)

    # 8. æ®µè½ã‚¿ã‚°: <p> â†’ ç©ºè¡ŒåŒºåˆ‡ã‚Š
    body = re.sub(r'<p[^>]*>(.*?)</p>', r'\n\n\1\n\n', body, flags=re.DOTALL)
    # æ®‹ã£ãŸé–‹å§‹ãƒ»çµ‚äº†ã‚¿ã‚°ã‚’å‰Šé™¤
    body = re.sub(r'</?p[^>]*>', '', body)

    # 9. ãƒªã‚¹ãƒˆã‚¿ã‚°: <ul>, <ol>, <li> â†’ Markdownãƒªã‚¹ãƒˆ
    # å˜ç´”ãªãƒªã‚¹ãƒˆã®å¤‰æ›ï¼ˆãƒã‚¹ãƒˆã¯è€ƒæ…®ã—ãªã„ï¼‰
    body = re.sub(r'<ul[^>]*>', '\n', body)
    body = re.sub(r'</ul>', '\n', body)
    body = re.sub(r'<ol[^>]*>', '\n', body)
    body = re.sub(r'</ol>', '\n', body)
    body = re.sub(r'<li[^>]*>(.*?)</li>', r'- \1\n', body, flags=re.DOTALL)
    # æ®‹ã£ãŸé–‰ã˜ã‚¿ã‚°ã ã‘ã®<li>ã‚’å‰Šé™¤
    body = re.sub(r'</li>', '', body)
    body = re.sub(r'<li[^>]*>', '- ', body)

    # 10. å¼·èª¿ã‚¿ã‚°: <strong>, <b> â†’ **...**
    body = re.sub(r'<strong[^>]*>(.*?)</strong>', r'**\1**', body, flags=re.DOTALL)
    body = re.sub(r'<b[^>]*>(.*?)</b>', r'**\1**', body, flags=re.DOTALL)

    # 11. æ–œä½“ã‚¿ã‚°: <em>, <i> â†’ *...*
    body = re.sub(r'<em[^>]*>(.*?)</em>', r'*\1*', body, flags=re.DOTALL)
    body = re.sub(r'<i[^>]*>(.*?)</i>', r'*\1*', body, flags=re.DOTALL)

    # 12. ã¯ã¦ãªè¨˜æ³•ã®è¦‹å‡ºã—: *... â†’ ##
    body = re.sub(r'^\*\*\*(.+)$', r'#### \1', body, flags=re.MULTILINE)
    body = re.sub(r'^\*\*(.+)$', r'### \1', body, flags=re.MULTILINE)
    body = re.sub(r'^\*([^*].+)$', r'## \1', body, flags=re.MULTILINE)

    # 13. ãƒªãƒ³ã‚¯: [url:title=text] â†’ [text](url)
    body = re.sub(r'\[([^\]]+):title=([^\]]+)\]', r'[\2](\1)', body)

    # 14. æ”¹è¡Œã‚¿ã‚°: <br />, <br>, <br/> â†’ ç©ºç™½ã¾ãŸã¯å‰Šé™¤
    body = re.sub(r'<br\s*/?\s*>', '\n\n', body, flags=re.IGNORECASE)

    # 15. HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
    body = html.unescape(body)

    # 16. æ®‹ã‚Šã®å…¨ã¦ã®HTMLã‚¿ã‚°ã‚’å‰Šé™¤
    body = re.sub(r'<[^>]+>', '', body)

    # 16.5. MDXã§å•é¡Œã¨ãªã‚‹æ–‡å­—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
    # è¦‹å‡ºã—å†…ã®{DRAFT}ãªã©ã®æ³¢æ‹¬å¼§ã‚’å‰Šé™¤ï¼ˆMDXã§ã¯{}ã¯ç‰¹åˆ¥ãªæ„å‘³ã‚’æŒã¤ï¼‰
    def escape_mdx_braces_in_headings(match):
        level = match.group(1)
        content = match.group(2)
        # æ³¢æ‹¬å¼§ã‚’å‰Šé™¤
        content = content.replace('{', '').replace('}', '')
        return f'{level} {content}'

    body = re.sub(r'^(#{1,6}) (.+)$', escape_mdx_braces_in_headings, body, flags=re.MULTILINE)

    # 17. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’2ã¤ã¾ã§ã«
    body = re.sub(r'\n{3,}', '\n\n', body)

    # 18. è¡Œæœ«ã®ç©ºç™½ã‚’å‰Šé™¤
    body = '\n'.join(line.rstrip() for line in body.split('\n'))

    return body.strip()

def extract_description(body, max_length=150):
    """æœ¬æ–‡ã‹ã‚‰èª¬æ˜æ–‡ã‚’æŠ½å‡º"""
    # HTMLã‚¿ã‚°ã‚’å‰Šé™¤
    text = re.sub(r'<[^>]+>', '', body)
    # Markdownè¨˜æ³•ã‚’å‰Šé™¤
    text = re.sub(r'[#\*\[\]!]', '', text)
    # æ”¹è¡Œã‚’å‰Šé™¤ã—ã¦ã‚¹ãƒšãƒ¼ã‚¹ã«
    text = re.sub(r'\n+', ' ', text)
    # é€£ç¶šã™ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()

    if len(text) > max_length:
        return text[:max_length] + '...'
    return text

def load_image_mapping(mapping_file, r2_public_url):
    """ç”»åƒãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    mapping = {}

    if not os.path.exists(mapping_file):
        print(f"âš ï¸  è­¦å‘Š: ç”»åƒãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {mapping_file}")
        return mapping

    with open(mapping_file, 'r', encoding='utf-8') as f:
        for line in f:
            if '|' in line:
                url, filename = line.strip().split('|')
                # URLã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰ã‚’æŠ½å‡ºã—ã¦ã¯ã¦ãªè¨˜æ³•IDã¨ã—ã¦ãƒãƒƒãƒ”ãƒ³ã‚°
                # ä¾‹: https://cdn-ak.f.st-hatena.com/images/fotolife/i/ippiki_koala/20221106/20221106195534.jpg
                # -> 20221106195534 ãŒIDã«ãªã‚‹
                basename = filename.rsplit('.', 1)[0]  # æ‹¡å¼µå­ã‚’é™¤å»
                mapping[url] = filename
                # ã¯ã¦ãªè¨˜æ³•ã® [f:id:...] ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚‚å¯¾å¿œ
                mapping[basename] = filename

    return mapping

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python3 convert_to_mdx.py <export_file> [r2_public_url]")
        print("Example: python3 convert_to_mdx.py ippikikoala.hatenablog.com.export.txt")
        sys.exit(1)

    export_file = sys.argv[1]
    r2_public_url = sys.argv[2] if len(sys.argv) > 2 else "https://pub-521ec77a6aeb44b18091baa73887e9b7.r2.dev"

    if not os.path.exists(export_file):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {export_file}")
        sys.exit(1)

    print(f"ğŸ“– ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {export_file}")
    print(f"ğŸŒ R2 Public URL: {r2_public_url}")
    print()

    # ç”»åƒãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿
    mapping_file = 'hatena_images/image_mapping.txt'
    image_mapping = load_image_mapping(mapping_file, r2_public_url)
    print(f"ğŸ“· ç”»åƒãƒãƒƒãƒ”ãƒ³ã‚°: {len(image_mapping)} ä»¶")
    print()

    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹
    print("ğŸ“„ è¨˜äº‹ã‚’è§£æä¸­...")
    posts = parse_hatena_export(export_file)
    print(f"âœ… {len(posts)} ä»¶ã®è¨˜äº‹ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
    print()

    # MDXãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    output_dir = Path('content/posts')
    output_dir.mkdir(parents=True, exist_ok=True)

    created = []
    for i, post in enumerate(posts, 1):
        slug, mdx_content = convert_to_mdx(post, image_mapping, r2_public_url)

        output_file = output_dir / f"{slug}.mdx"

        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        if output_file.exists():
            backup_file = output_dir / f"{slug}.mdx.bak"
            output_file.rename(backup_file)
            print(f"ğŸ”„ [{i}/{len(posts)}] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {output_file.name}.bak")

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(mdx_content)

        title = post.get('title', 'Untitled')
        print(f"âœ… [{i}/{len(posts)}] ä½œæˆ: {output_file.name} - {title}")
        created.append(str(output_file))

    print("\n" + "="*60)
    print(f"âœ… å¤‰æ›å®Œäº†: {len(created)} ä»¶")
    print(f"ğŸ“ ä¿å­˜å…ˆ: {output_dir}/")
    print("="*60)
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. npm run dev ã§å‹•ä½œç¢ºèª")
    print("2. ç”»åƒãŒè¡¨ç¤ºã•ã‚Œãªã„å ´åˆã¯ç”»åƒã‚’R2ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    print("3. å„è¨˜äº‹ã‚’é–‹ã„ã¦å†…å®¹ã‚’ç¢ºèª")
