#!/usr/bin/env python3
"""
ã¯ã¦ãªãƒ–ãƒ­ã‚°ã‹ã‚‰ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒç”»åƒã‚’å–å¾—ã—ã€MDXãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã™ã‚‹

æ–¹æ³•: å„è¨˜äº‹ã®HTMLãƒšãƒ¼ã‚¸ã‹ã‚‰og:imageã‚’å–å¾—

Usage:
    python3 scripts/update_eyecatch.py
"""

import requests
import xml.etree.ElementTree as ET
import os
import re
from pathlib import Path
from urllib.parse import unquote
import time

# è¨­å®š
HATENA_ID = "ippiki_koala"
BLOG_DOMAIN = "ippikikoala.hatenablog.com"
API_KEY = "eh7uobkq38"
R2_BASE_URL = "https://pub-521ec77a6aeb44b18091baa73887e9b7.r2.dev/posts"
CONTENT_DIR = Path(__file__).parent.parent / "content" / "posts"
MAPPING_FILE = Path(__file__).parent.parent / "hatena_images" / "image_mapping.txt"

# XMLåå‰ç©ºé–“
NAMESPACES = {
    'atom': 'http://www.w3.org/2005/Atom',
    'app': 'http://www.w3.org/2007/app',
}


def load_image_mapping():
    """image_mapping.txtã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã®ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    filenames = set()
    if MAPPING_FILE.exists():
        with open(MAPPING_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if '|' in line:
                    _, filename = line.split('|', 1)
                    filenames.add(filename)
    print(f"ğŸ“· R2ã«ã‚ã‚‹ç”»åƒ: {len(filenames)} ä»¶")
    return filenames


def fetch_hatena_entries():
    """ã¯ã¦ãªãƒ–ãƒ­ã‚°APIã‹ã‚‰å…¨è¨˜äº‹ã®URLä¸€è¦§ã‚’å–å¾—"""
    entries = []
    url = f"https://blog.hatena.ne.jp/{HATENA_ID}/{BLOG_DOMAIN}/atom/entry"

    while url:
        print(f"ğŸ“¥ å–å¾—ä¸­: {url}")
        response = requests.get(url, auth=(HATENA_ID, API_KEY))

        if response.status_code != 200:
            print(f"âŒ APIã‚¨ãƒ©ãƒ¼: {response.status_code}")
            break

        root = ET.fromstring(response.content)

        for entry in root.findall('atom:entry', NAMESPACES):
            title = entry.find('atom:title', NAMESPACES)
            published = entry.find('atom:published', NAMESPACES)

            # è¨˜äº‹ã®URLï¼ˆalternate linkï¼‰ã‚’å–å¾—
            alternate_link = entry.find("atom:link[@rel='alternate']", NAMESPACES)
            entry_url = alternate_link.get('href') if alternate_link is not None else None

            if title is not None and published is not None and entry_url:
                entry_data = {
                    'title': title.text or '',
                    'published': published.text or '',
                    'url': entry_url
                }
                entries.append(entry_data)

        next_link = root.find("atom:link[@rel='next']", NAMESPACES)
        url = next_link.get('href') if next_link is not None else None

    print(f"âœ… å–å¾—å®Œäº†: {len(entries)} ä»¶ã®è¨˜äº‹")
    return entries


def get_eyecatch_from_page(page_url):
    """è¨˜äº‹ãƒšãƒ¼ã‚¸ã‹ã‚‰og:imageã‚’å–å¾—ã—ã€å®Ÿéš›ã®ç”»åƒURLã‚’æŠ½å‡º"""
    try:
        response = requests.get(page_url, timeout=10)
        if response.status_code != 200:
            return None

        # og:imageã‚’æ­£è¦è¡¨ç¾ã§å–å¾—
        match = re.search(r'<meta property="og:image" content="([^"]+)"', response.text)
        if not match:
            return None

        og_image_url = match.group(1)

        # ã¯ã¦ãªã®ç”»åƒå¤‰æ›URLã‹ã‚‰å®Ÿéš›ã®URLã‚’æŠ½å‡º
        # ä¾‹: https://cdn.image.st-hatena.com/.../https%3A%2F%2Fcdn-ak.f.st-hatena.com%2F...
        inner_match = re.search(r'https%3A%2F%2Fcdn-ak\.f\.st-hatena\.com[^"&]+', og_image_url)
        if inner_match:
            return unquote(inner_match.group(0))

        # ç›´æ¥ã®ã¯ã¦ãªCDN URLã®å ´åˆ
        if 'cdn-ak.f.st-hatena.com' in og_image_url:
            return og_image_url

        return None
    except Exception as e:
        print(f"    âš ï¸ ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def extract_filename_from_hatena_url(hatena_url):
    """ã¯ã¦ãªCDN URLã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡º"""
    # ä¾‹: https://cdn-ak.f.st-hatena.com/images/fotolife/i/ippiki_koala/20260104/20260104233503.jpg
    match = re.search(r'/(\d{14}\.(?:jpg|jpeg|png|gif))$', hatena_url, re.IGNORECASE)
    if match:
        return match.group(1)
    return None


def parse_published_date(published_str):
    """ISO 8601å½¢å¼ã®æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹"""
    try:
        return published_str[:10]  # YYYY-MM-DD
    except:
        return None


def find_mdx_file(date_str, title):
    """æ—¥ä»˜ã¨ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰MDXãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™"""
    if not CONTENT_DIR.exists():
        return None

    for mdx_file in CONTENT_DIR.glob(f"{date_str}*.mdx"):
        with open(mdx_file, 'r', encoding='utf-8') as f:
            content = f.read()

        title_match = re.search(r'^title:\s*["\'](.+?)["\']', content, re.MULTILINE)
        if title_match:
            file_title = title_match.group(1)
            if file_title == title or title in file_title or file_title in title:
                return mdx_file

    return None


def update_mdx_image(mdx_file, r2_url):
    """MDXãƒ•ã‚¡ã‚¤ãƒ«ã®imageãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ›´æ–°"""
    with open(mdx_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # æ—¢ã«imageãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    current_image_match = re.search(r'^image:\s*["\'](.+?)["\']', content, re.MULTILINE)
    if current_image_match and current_image_match.group(1).strip():
        return False, "already_set"

    # image: "" ã‚’æ›´æ–°
    new_content = re.sub(
        r'^image:\s*["\']["\']',
        f'image: "{r2_url}"',
        content,
        flags=re.MULTILINE
    )

    if new_content != content:
        with open(mdx_file, 'w', encoding='utf-8') as f:
            f.write(new_content)
        return True, "updated"

    return False, "no_change"


def main():
    print("=" * 60)
    print("ã¯ã¦ãªãƒ–ãƒ­ã‚° ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒç”»åƒ æ›´æ–°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    print()

    # R2ã«ã‚ã‚‹ç”»åƒã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
    r2_filenames = load_image_mapping()

    # ã¯ã¦ãªãƒ–ãƒ­ã‚°APIã‹ã‚‰è¨˜äº‹URLä¸€è¦§ã‚’å–å¾—
    print()
    entries = fetch_hatena_entries()

    if not entries:
        print("âŒ è¨˜äº‹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return

    print()
    print("=" * 60)
    print("å„è¨˜äº‹ã®ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒã‚’å–å¾—ã—ã¦MDXã‚’æ›´æ–°ä¸­...")
    print("=" * 60)
    print()

    stats = {
        'updated': 0,
        'already_set': 0,
        'no_eyecatch': 0,
        'not_in_r2': 0,
        'mdx_not_found': 0,
    }

    not_in_r2_list = []

    for i, entry in enumerate(entries, 1):
        title = entry['title']
        published = entry['published']
        page_url = entry['url']

        date_str = parse_published_date(published)
        if not date_str:
            continue

        print(f"[{i}/{len(entries)}] {title[:40]}...")

        # è¨˜äº‹ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒURLã‚’å–å¾—
        eyecatch_url = get_eyecatch_from_page(page_url)

        if not eyecatch_url:
            print(f"    â­ï¸ ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒãªã—")
            stats['no_eyecatch'] += 1
            continue

        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡º
        filename = extract_filename_from_hatena_url(eyecatch_url)

        if not filename:
            print(f"    âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«åæŠ½å‡ºå¤±æ•—: {eyecatch_url}")
            stats['no_eyecatch'] += 1
            continue

        # R2ã«ã‚ã‚‹ã‹ç¢ºèª
        if filename not in r2_filenames:
            stats['not_in_r2'] += 1
            not_in_r2_list.append({
                'title': title,
                'date': date_str,
                'hatena_url': eyecatch_url,
                'filename': filename
            })
            print(f"    âŒ R2ã«ãªã„: {filename}")
            continue

        r2_url = f"{R2_BASE_URL}/{filename}"

        # MDXãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        mdx_file = find_mdx_file(date_str, title)

        if not mdx_file:
            stats['mdx_not_found'] += 1
            print(f"    âš ï¸ MDXãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
            continue

        # MDXã‚’æ›´æ–°
        updated, status = update_mdx_image(mdx_file, r2_url)

        if status == 'updated':
            stats['updated'] += 1
            print(f"    âœ… æ›´æ–°å®Œäº†")
        elif status == 'already_set':
            stats['already_set'] += 1
            print(f"    â­ï¸ æ—¢ã«è¨­å®šæ¸ˆã¿")

        # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚å°‘ã—å¾…æ©Ÿ
        time.sleep(0.3)

    # çµæœè¡¨ç¤º
    print()
    print("=" * 60)
    print("çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 60)
    print(f"âœ… æ›´æ–°æˆåŠŸ: {stats['updated']} ä»¶")
    print(f"â­ï¸  æ—¢ã«è¨­å®šæ¸ˆã¿: {stats['already_set']} ä»¶")
    print(f"ğŸ“· ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒãªã—: {stats['no_eyecatch']} ä»¶")
    print(f"âŒ R2ã«ç”»åƒãªã—: {stats['not_in_r2']} ä»¶")
    print(f"ğŸ“„ MDXãƒ•ã‚¡ã‚¤ãƒ«ãªã—: {stats['mdx_not_found']} ä»¶")

    # R2ã«ãªã„ç”»åƒã‚’è¡¨ç¤º
    if not_in_r2_list:
        print()
        print("=" * 60)
        print("âš ï¸  R2ã«è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒç”»åƒ:")
        print("=" * 60)
        for item in not_in_r2_list:
            print(f"  ğŸ“… {item['date']} | {item['title'][:40]}...")
            print(f"     ãƒ•ã‚¡ã‚¤ãƒ«å: {item['filename']}")
            print(f"     URL: {item['hatena_url']}")
            print()


if __name__ == "__main__":
    main()
